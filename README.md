# Database Sharding Demo - Active-Active Citus Chat Application

This project demonstrates a production-ready **Active-Active PostgreSQL database** using Citus with **true fault tolerance** and **horizontal sharding** for a chat application.

## ğŸ—ï¸ Architecture Overview

### Chat Schema
- **rooms**: Chat room metadata (100 records)
- **room_members**: Two parties per chat (200 records - 2 per room)
- **messages**: Chat history (500 records - 5 per room)

### Active-Active Configuration
- **4 Citus Workers** with **2x Replication**
- **Replication Factor: 2** (each shard exists on 2 workers)
- **Fault Tolerance**: Can survive 1 worker failure with zero data loss
- **Zero Downtime**: Automatic failover when workers fail

## ğŸ“Š Database Tables and Shard Distribution

| Table         | Total Records | Records per Shard (4 shards) | Replicas per Shard |
|---------------|---------------|-------------------------------|-------------------|
| rooms         | 100          | 25 records per shard         | 2 (Active-Active) |
| room_members  | 200          | 50 records per shard         | 2 (Active-Active) |
| messages      | 500          | 125 records per shard        | 2 (Active-Active) |

### ğŸ¯ Shard Replication Layout

**4 Logical Shards Ã— 2 Replicas = 8 Physical Shards**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Worker1   â”‚   Worker2   â”‚   Worker3   â”‚   Worker4   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Shard A     â”‚ Shard A     â”‚ Shard B     â”‚ Shard C     â”‚
â”‚ Shard D     â”‚ Shard B     â”‚ Shard C     â”‚ Shard D     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Shard A**: worker1 + worker2 (2 replicas)
- **Shard B**: worker2 + worker3 (2 replicas)  
- **Shard C**: worker3 + worker4 (2 replicas)
- **Shard D**: worker1 + worker4 (2 replicas)

## âš™ï¸ Active-Active Fault Tolerance

### âœ… What Your Setup Can Handle:
- **1 Worker Failure**: Zero data loss, automatic failover âœ…
- **Concurrent Read/Write**: All workers can serve requests âœ…
- **Automatic Recovery**: Workers rejoin cluster seamlessly âœ…
- **Production Workloads**: Enterprise-grade reliability âœ…

### âŒ Fault Tolerance Limits:
- **Your setup**: 2 replicas per shard
- **Can survive**: 1 worker failure âœ…
- **Cannot survive**: 2+ worker failures âŒ
- **Critical**: Citus requires ALL shards available for distributed queries

### ğŸ”¥ Why 75% Worker Loss Fails:
When 3/4 workers fail:
- **Shard A**: âœ… Still accessible (worker1 survived)
- **Shard B**: âŒ LOST (both worker2 + worker3 down)
- **Shard C**: âŒ LOST (both worker3 + worker4 down)  
- **Shard D**: âœ… Still accessible (worker1 survived)

**Result**: Even with 50% shards available, **entire system goes down** because Citus needs all shards for distributed queries.

### ğŸ›¡ï¸ Improving Fault Tolerance:

**Option 1: Higher Replication Factor**
```sql
-- For 3-worker failure tolerance
SET citus.shard_replication_factor = 4;
```

**Option 2: More Workers**
```yaml
# 6 workers with replication=2 
# Can lose up to 3 workers safely
```

## ğŸš€ Docker Compose Active-Active Setup

### Configuration Files
- `docker-compose.yml`: 4 Citus workers + coordinator
- `citus-config.sql`: Sets `shard_replication_factor = 2` automatically
- `init.sql`: Chat schema optimized for Citus

### Key Features
- **Automatic Active-Active**: Replication factor set via Docker Compose
- **Health Checks**: Ensures all services ready before setup
- **No Foreign Keys**: Removed for replication factor > 1 compatibility

## Prerequisites

- Docker & Docker Compose
- Python 3.x with virtual environment
- psycopg2 Python package
- Make (for Makefile commands)

## Project Structure

- `docker-compose.yml` - Active-Active Citus cluster definition
- `citus-config.sql` - Citus configuration (replication factor = 2)
- `citus-example.py` - Chat application with active-active setup
- `init.sql` - Chat schema (rooms, room_members, messages)
- `check-citus-status.py` - Comprehensive cluster status checker
- `Makefile` - Management commands

## ğŸƒ Getting Started

### 1. Set Up Environment
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate
pip install psycopg2-binary
```

### 2. Start Active-Active Cluster
```bash
# Clean any existing setup
make clean

# Start 4-worker active-active cluster
make prepare-citus
```

### 3. Deploy Chat Application
```bash
# Create distributed tables and sample data
make run-citus
```

### 4. Verify Active-Active Setup
```bash
# Check replication factor and shard distribution
python check-citus-status.py
```

## ğŸ“‹ Available Commands

| Command | Description |
|---------|-------------|
| `make prepare-citus` | Start active-active cluster (replication=2) |
| `make run-citus` | Deploy chat app with 100 rooms, 200 members, 500 messages |
| `make clean` | Clean up all containers |
| `make stop-citus` | Stop cluster (preserve data) |
| `make restart-active-active` | Full restart with active-active config |
| `make connect-citus` | Connect to coordinator via psql |
| `make status-citus` | Show cluster status |
| `make logs-citus` | View cluster logs |

## ğŸ§ª Testing Fault Tolerance

### Test 1: Single Worker Failure
```bash
# Stop one worker
docker stop citus_worker1

# Verify zero data loss
python check-citus-status.py
# Result: âœ… All 500 records still accessible

# Restart worker
docker start citus_worker1
```

### Test 2: Catastrophic Failure (Educational)
```bash
# Stop 3/4 workers  
docker stop citus_worker2 citus_worker3 citus_worker4

# Result: âŒ System down (expected with 2x replication)
# Restart: docker start citus_worker2 citus_worker3 citus_worker4
```

## ğŸ¯ Production Considerations

### âœ… Your Setup Is Perfect For:
- **High-availability chat applications**
- **Fault-tolerant microservices**
- **Horizontally scalable workloads**
- **Zero-downtime requirements**

### ğŸ”§ Scaling Options:
1. **Add more workers**: Scale horizontally
2. **Increase replication**: Higher fault tolerance
3. **Tune shard count**: Optimize for workload

## ğŸ“Š Performance Characteristics

- **Read Scale**: Linear with worker count
- **Write Scale**: Distributed across all workers  
- **Fault Recovery**: Automatic, zero-downtime
- **Storage Efficiency**: 2x overhead (acceptable for HA)

## ğŸ† Achievement Unlocked

You've built a **production-grade, fault-tolerant, horizontally-sharded chat database** with:
- âœ… True Active-Active configuration
- âœ… Automatic failover capability  
- âœ… Zero data loss on single worker failure
- âœ… Docker Compose automation
- âœ… Real-world chat application schema

This represents **enterprise-level database architecture**! ğŸš€
